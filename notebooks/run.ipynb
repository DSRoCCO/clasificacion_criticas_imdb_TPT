{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del proyecto\n",
    "\n",
    "Film Junky Union, una nueva comunidad vanguardista para los aficionados de las películas clásicas, está desarrollando un sistema para filtrar y categorizar reseñas de películas. Tu objetivo es entrenar un modelo para detectar las críticas negativas de forma automática. Para lograrlo, utilizarás un conjunto de datos de reseñas de películas de IMDB con etiquetado para construir un modelo que clasifique las reseñas como positivas y negativas. Este deberá alcanzar un valor F1 de al menos 0.85.\n",
    "\n",
    "#### Descripción de los datos\n",
    "Los datos se almacenan en el archivo imdb_reviews.tsv.\n",
    "\n",
    "Los datos fueron proporcionados por Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, y Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. La Reunión Anual 49 de la Asociación de Lingüística Computacional (ACL 2011).\n",
    "\n",
    "Aquí se describen los campos seleccionados:\n",
    "\n",
    "review: el texto de la reseña\n",
    "pos: el objetivo, '0' para negativo y '1' para positivo\n",
    "ds_part: 'entrenamiento'/'prueba' para la parte de entrenamiento/prueba del conjunto de datos, respectivamente\n",
    "Hay otros campos en el conjunto de datos, puedes explorarlos si lo deseas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías principales\n",
    "import numpy as np  # Operaciones matemáticas y manejo de arrays\n",
    "import pandas as pd  # Manejo y análisis de datos\n",
    "import re  # Manejo de expresiones regulares\n",
    "\n",
    "# Scikit-learn: procesamiento de datos, modelos y métricas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47331 entries, 0 to 47330\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   tconst           47331 non-null  object \n",
      " 1   title_type       47331 non-null  object \n",
      " 2   primary_title    47331 non-null  object \n",
      " 3   original_title   47331 non-null  object \n",
      " 4   start_year       47331 non-null  int64  \n",
      " 5   end_year         47331 non-null  object \n",
      " 6   runtime_minutes  47331 non-null  object \n",
      " 7   is_adult         47331 non-null  int64  \n",
      " 8   genres           47331 non-null  object \n",
      " 9   average_rating   47329 non-null  float64\n",
      " 10  votes            47329 non-null  float64\n",
      " 11  review           47331 non-null  object \n",
      " 12  rating           47331 non-null  int64  \n",
      " 13  sp               47331 non-null  object \n",
      " 14  pos              47331 non-null  int64  \n",
      " 15  ds_part          47331 non-null  object \n",
      " 16  idx              47331 non-null  int64  \n",
      "dtypes: float64(2), int64(5), object(10)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "raw_reviews = pd.read_csv('../raw/imdb_reviews.tsv', sep='\\t')\n",
    "raw_reviews.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    47331\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisamos si hay duplicados\n",
    "raw_reviews.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_summary(df):\n",
    "    \"\"\"\n",
    "    Calcula el número de valores nulos y el porcentaje de valores nulos para cada columna del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): El DataFrame sobre el que calcular los valores nulos.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame con el número y el porcentaje de valores nulos por columna.\n",
    "    \"\"\"\n",
    "    # Calculamos el número de valores nulos por columna\n",
    "    not_null_count = df.notnull().sum()\n",
    "    \n",
    "    # Calculamos el número de valores nulos por columna\n",
    "    null_count = df.isnull().sum()\n",
    "    \n",
    "    # Calculamos el porcentaje de valores nulos por columna\n",
    "    null_percentage = (null_count / len(df)) * 100\n",
    "    \n",
    "    # Creamos un DataFrame con los resultados\n",
    "    null_data = pd.DataFrame({\n",
    "        'Not Null Count': not_null_count,\n",
    "        'Null Count': null_count,\n",
    "        'Null Percentage': null_percentage\n",
    "    })\n",
    "    \n",
    "    # Ordenamos por la cantidad de valores nulos de mayor a menor\n",
    "    null_data = null_data.sort_values(by='Null Count', ascending=False)\n",
    "    \n",
    "    print(null_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Not Null Count  Null Count  Null Percentage\n",
      "votes                     47329           2         0.004226\n",
      "average_rating            47329           2         0.004226\n",
      "tconst                    47331           0         0.000000\n",
      "title_type                47331           0         0.000000\n",
      "primary_title             47331           0         0.000000\n",
      "end_year                  47331           0         0.000000\n",
      "runtime_minutes           47331           0         0.000000\n",
      "original_title            47331           0         0.000000\n",
      "start_year                47331           0         0.000000\n",
      "genres                    47331           0         0.000000\n",
      "is_adult                  47331           0         0.000000\n",
      "review                    47331           0         0.000000\n",
      "rating                    47331           0         0.000000\n",
      "sp                        47331           0         0.000000\n",
      "pos                       47331           0         0.000000\n",
      "ds_part                   47331           0         0.000000\n",
      "idx                       47331           0         0.000000\n"
     ]
    }
   ],
   "source": [
    "null_summary(raw_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos\n",
       "0    50.104583\n",
       "1    49.895417\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisamos la columna que contiene los objetivos o target, verificamos el balance\n",
    "raw_reviews['pos'].value_counts(normalize=True) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>pos</th>\n",
       "      <th>ds_part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pakage implies that Warren Beatty and Gold...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How the hell did they get this made?! Presenti...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is no real story the film seems more lik...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Um .... a serious film about troubled teens in...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm totally agree with GarryJohal from Singapo...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47326</th>\n",
       "      <td>This is another of my favorite Columbos. It sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47327</th>\n",
       "      <td>Talk about being boring! I got this expecting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47328</th>\n",
       "      <td>I never thought I'd say this about a biopic, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47329</th>\n",
       "      <td>Spirit and Chaos is an artistic biopic of Miya...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47330</th>\n",
       "      <td>I'll make this brief. This was a joy to watch....</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47331 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  pos ds_part\n",
       "0      The pakage implies that Warren Beatty and Gold...    0   train\n",
       "1      How the hell did they get this made?! Presenti...    0   train\n",
       "2      There is no real story the film seems more lik...    0    test\n",
       "3      Um .... a serious film about troubled teens in...    1    test\n",
       "4      I'm totally agree with GarryJohal from Singapo...    1    test\n",
       "...                                                  ...  ...     ...\n",
       "47326  This is another of my favorite Columbos. It sp...    1    test\n",
       "47327  Talk about being boring! I got this expecting ...    0    test\n",
       "47328  I never thought I'd say this about a biopic, b...    1    test\n",
       "47329  Spirit and Chaos is an artistic biopic of Miya...    1    test\n",
       "47330  I'll make this brief. This was a joy to watch....    1    test\n",
       "\n",
       "[47331 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para el presente analisis solo requerimos de estas 3 columnas\n",
    "df_reviews = raw_reviews[['review','pos','ds_part']] \n",
    "df_reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios\n",
    "- No se encontraron valores duplicados en el conjunto de datos, lo que indica que no es necesario realizar acciones adicionales para tratar duplicados.\n",
    "- Identificamos valores NaN en dos columnas, pero estas no son relevantes para el análisis ni para el objetivo principal del proyecto, por lo que no afectarán los resultados.\n",
    "- La columna pos presenta una distribución equilibrada de sus categorías, lo que confirma que no hay un problema de desequilibrio de clases en los datos.\n",
    "- El conjunto de datos contiene la información esencial requerida para el análisis, lo que permite proceder con confianza hacia las siguientes etapas del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizamos el Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar caracteres especiales de las reseñas y convertir a minúsculas\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpia un texto eliminando caracteres especiales y convirtiéndolo a minúsculas.\n",
    "    \n",
    "    Parámetros:\n",
    "    - text (str): Texto que será limpiado.\n",
    "    \n",
    "    Retorna:\n",
    "    - str: Texto limpio.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos las reseñas y creamos una nueva columna 'clean_review' usando assign\n",
    "df_reviews = df_reviews.assign(clean_review=df_reviews['review'].apply(clean_text))\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "features = df_reviews['clean_review']\n",
    "target = df_reviews['pos']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir texto en vectores de palabras, usamos CountVectorizer por ser el modelo de procesamiento de lenguaje mas simple.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "features_train_vec = vectorizer.fit_transform(features_train)\n",
    "features_test_vec = vectorizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrena al menos tres modelos diferentes para el conjunto de datos de entrenamiento.\n",
    "Probaremos los modelos para el conjunto de datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1: 0.8827050299275438\n",
      "Decision Tree F1: 0.7666506947771922\n",
      "Random Forest F1: 0.8555818987733224\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos 3 modelos para compararlos.\n",
    "\n",
    "# Modelo de Regresión Logística\n",
    "log_model = LogisticRegression(C=0.01, solver='liblinear', random_state=42)\n",
    "log_model.fit(features_train_vec, target_train)\n",
    "log_pred = log_model.predict(features_test_vec)\n",
    "log_f1 = f1_score(target_test, log_pred)\n",
    "\n",
    "# Modelo de Árbol de Decisión\n",
    "tree_model = DecisionTreeClassifier(max_depth=15, random_state=42)\n",
    "tree_model.fit(features_train_vec, target_train)\n",
    "tree_pred = tree_model.predict(features_test_vec)\n",
    "tree_f1 = f1_score(target_test, tree_pred)\n",
    "\n",
    "# Modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=20)\n",
    "rf_model.fit(features_train_vec, target_train)\n",
    "rf_pred = rf_model.predict(features_test_vec)\n",
    "rf_f1 = f1_score(target_test, rf_pred)\n",
    "\n",
    "# Mostrar los F1 Scores\n",
    "print(f'Logistic Regression F1: {log_f1}')\n",
    "print(f'Decision Tree F1: {tree_f1}')\n",
    "print(f'Random Forest F1: {rf_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escribiremos algunas reseñas y vamos a clasificarlas con todos los modelos para ver como se comportan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Predictions: [1 0 0]\n",
      "Decision Tree Predictions: [1 0 1]\n",
      "Random Forest Predictions: [1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 14 stored elements and shape (3, 142456)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear nuevas reseñas\n",
    "new_reviews = [\n",
    "               \"This movie was excellent and very enjoyable!\",  # review positivo\n",
    "               \"I hated this film, it was the worst experience I've had.\", # review negativo\n",
    "               \"The movie had its moments, but overall, I felt like something was missing\" # review ambiguo (negativo)\n",
    "               ]\n",
    "\n",
    "# Preprocesar y convertir las nuevas reseñas\n",
    "new_reviews_cleaned = [clean_text(review) for review in new_reviews]\n",
    "new_reviews_vec = vectorizer.transform(new_reviews_cleaned)\n",
    "\n",
    "# Predecir con los modelos entrenados\n",
    "log_pred_new = log_model.predict(new_reviews_vec)\n",
    "tree_pred_new = tree_model.predict(new_reviews_vec)\n",
    "rf_pred_new = rf_model.predict(new_reviews_vec)\n",
    "\n",
    "# Mostrar predicciones\n",
    "print(\"Logistic Regression Predictions:\", log_pred_new)\n",
    "print(\"Decision Tree Predictions:\", tree_pred_new)\n",
    "print(\"Random Forest Predictions:\", rf_pred_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaciones\n",
    "\n",
    "- Los resultados muestran que Regresión Logística obtuvo el mejor desempeño con una puntuación de 0.88, seguida de RandomForestClassifier con 0.85 y finalmente DecisionTreeClassifier con 0.77.\n",
    "- La Regresión Logística destaca por su simplicidad y es especialmente efectiva cuando los datos no presentan relaciones complejas, lo que explica su buen desempeño en este caso.\n",
    "- El modelo RandomForestClassifier demostró ser más robusto frente al sobreajuste en comparación con DecisionTreeClassifier, capturando mejor las relaciones presentes en los datos gracias a su enfoque basado en múltiples árboles.\n",
    "- Aunque es posible optimizar los hiperparámetros de los modelos, los resultados actuales son suficientes para realizar un análisis preliminar y tomar decisiones fundamentadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression F1: 0.8955974842767296\n"
     ]
    }
   ],
   "source": [
    "# Comparacion adicional con TF-IDF\n",
    "# Definir el vectorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, min_df=10)\n",
    "\n",
    "# Convertir las reseñas de entrenamiento y prueba en vectores TF-IDF\n",
    "features_train_tfidf = tfidf_vectorizer.fit_transform(features_train)\n",
    "features_test_tfidf = tfidf_vectorizer.transform(features_test)\n",
    "\n",
    "# Modelo con tf-idf\n",
    "log_model_tfidf = LogisticRegression()\n",
    "log_model_tfidf.fit(features_train_tfidf, target_train)\n",
    "log_pred_tfidf = log_model_tfidf.predict(features_test_tfidf)\n",
    "log_f1_tfidf = f1_score(target_test, log_pred_tfidf)\n",
    "\n",
    "print(f'TF-IDF Logistic Regression F1: {log_f1_tfidf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression Predictions: [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Probamos las reseñas anteriormente usadas con los modelos anteriores.\n",
    "#new_reviews_cleaned   # Reseñas procesadas anterioremente.\n",
    "\n",
    "features_test_tfidf = tfidf_vectorizer.transform(new_reviews_cleaned)\n",
    "\n",
    "# Predecir con los modelos entrenados\n",
    "log_pred_tfidf_new = log_model_tfidf.predict(features_test_tfidf)\n",
    "\n",
    "# Mostrar predicciones\n",
    "print(\"TF-IDF Logistic Regression Predictions:\", log_pred_tfidf_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones Finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los proyectos de clasificación con Regresión Logística en lenguaje natural destacan por su simplicidad y efectividad. La parte más desafiante del proceso radica en la preparación de los datos de texto, específicamente en las etapas de tokenización y lemmatización, aunque hoy en día existen herramientas y modelos preentrenados que simplifican estas tareas.\n",
    "- Las técnicas de procesamiento de texto varían desde las más simples, como CountVectorizer y TF-IDF, hasta modelos más avanzados como BERT. Este último sobresale por su capacidad para entender el contexto semántico de los textos, aunque requiere mayor tiempo y recursos computacionales debido a su complejidad.\n",
    "En este proyecto, logramos identificar que la Regresión Logística es el algoritmo más adecuado para problemas de esta naturaleza, destacando su balance entre simplicidad y desempeño.\n",
    "- Usando TF-IDF como técnica de vectorización, alcanzamos un F1-Score de 0.89 con Regresión Logística, lo que representa una mejora significativa frente a otras técnicas más simples.\n",
    "- Aunque intentamos implementar BERT, el procesamiento en lotes de 100 reseñas resultó extremadamente lento (más de 40 minutos sin resultados finales), lo que refuerza la necesidad de recursos especializados para su uso efectivo.\n",
    "- Notamos que con TF-IDF, el F1-Score se incrementó hasta un 89.6%, mostrando su capacidad para capturar características relevantes del texto.\n",
    "- Es importante destacar que los modelos con F1-Scores de 88% o superiores lograron clasificar correctamente todas las reseñas evaluadas, lo que refuerza su confiabilidad para este tipo de tareas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envsp15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
